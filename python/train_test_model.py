import argparse
import numpy as np
import pickle
from sklearn.metrics import classification_report
import sys
import tensorflow as tf
from tensorflow.keras.models import load_model

# --------------------------------------------------------------------------------------------------
# Parsing Command Line Arguments and Initializing Constants
# --------------------------------------------------------------------------------------------------

p = argparse.ArgumentParser(description='Train\Test a Neural Network model on a dataset.')
p.add_argument("mode", type=str, choices=["train", "test", "traintest"],
               default="train", help="whether to train a new model, test an old one, or do both")
p.add_argument("prefix", type=str, help="prefix for all the files generated by setup_dataset.py")
# p.add_argument("data_type", type=str, choices=["names", "commens", "nc"],
#                default="nc", help="type of the information recorded in the dataset")
p.add_argument("-emb_dim", type=int, default=128, required=False, help="embedding size")
p.add_argument("-batch_size", type=int, default=128, required=False, help="batch size")
p.add_argument("-epochs", type=int, default=50, required=False, help="maximum number of epochs")
p.add_argument("-lr", type=float, default=1e-4, required=False, help="base learning rate")
p.add_argument("-hidden_units_lstm", type=int, default=64, required=False,
               help="number of LSTM hidden units")
p.add_argument("-hidden_units_dense", type=int, default=64, required=False,
               help="number of ReLU units")
p.add_argument("--load_model", type=bool, default=False, required=False,
               help="if true, start training from a preexisting model")

args = p.parse_args(sys.argv[1:])
print(args)

with open(args.prefix + "tokenizer.pkl", 'rb') as f:
    tokenizer = pickle.load(f)
    vocab_size = max(tokenizer.index_word.keys())
with open(args.prefix + "label_dict.pkl", 'rb') as f:
    label_to_idx, _ = pickle.load(f)
    num_labels = len(label_to_idx)

# --------------------------------------------------------------------------------------------------
# Training the Model (Optional)
# --------------------------------------------------------------------------------------------------

if args.mode != 'test':

    with open(args.prefix + "train_set.pkl", 'rb') as f:
        train_ds = tf.data.Dataset.from_tensor_slices(pickle.load(f))
    with open(args.prefix + "dev_set.pkl", 'rb') as f:
        dev_ds = tf.data.Dataset.from_tensor_slices(pickle.load(f))

    train_ds = train_ds.shuffle(20000).batch(args.batch_size)
    dev_ds = dev_ds.shuffle(20000).batch(args.batch_size)

    if args.load_model:
        model = load_model(args.prefix + 'model.h5')
    else:
        # initializing the model
        model = tf.keras.Sequential([
            tf.keras.layers.Embedding(vocab_size + 1, args.emb_dim, mask_zero=True),
            tf.keras.layers.LSTM(args.hidden_units_lstm),
            tf.keras.layers.Dense(args.hidden_units_dense, activation='relu'),
            tf.keras.layers.Dense(num_labels + 1)
        ])

        model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
                      optimizer=tf.keras.optimizers.Adam(args.lr),
                      metrics=['accuracy'])

    early_stopping_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss',
                                                               mode='min',
                                                               patience=2)
    model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(
        filepath=args.prefix + 'model.h5',
        save_weights_only=False,
        monitor='val_loss',
        mode='min',
        save_best_only=True)

    model.fit(train_ds, epochs=args.epochs, validation_data=dev_ds, shuffle=True,
              callbacks=[early_stopping_callback, model_checkpoint_callback])

# --------------------------------------------------------------------------------------------------
# Testing the Model (Optional)
# --------------------------------------------------------------------------------------------------

if args.mode != "train":
    with open(args.prefix + "test_set.pkl", 'rb') as f:
        test_xs, test_ys = pickle.load(f)

    if args.mode == "test":  # old model must be loaded
        model = load_model(args.prefix + 'model.h5')

    y_preds = model.predict(test_xs)
    y_preds_bool = np.argmax(y_preds, axis=1)
    print(classification_report(test_ys, y_preds_bool))